{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_census_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBHMlASYZJYtyHiX/TVa2s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urZOIwgHXBLB"
      },
      "source": [
        "#Tutorial overview\n",
        "\n",
        "L'obiettivo di questo tutorial è quello di mostrare come si costruisce una semplice pipeline di machine learning, partendo dal caricamento dati fino al training e l'inferenza di un modello.\n",
        "\n",
        "## Tematiche\n",
        "In questo tutorial vedremo:\n",
        "* Download e processing dei dati;\n",
        "* Design e training di un modello tramite l'utilizzo di Keras;\n",
        "* Visualizzare il training del modello.\n",
        "\n",
        "## Caso d'uso\n",
        "Il **caso d'uso** su cui lavoreremo si basa sul [United States Census Income\n",
        "Dataset](https://archive.ics.uci.edu/ml/datasets/census+income), fornito dalla [UC Irvine Machine Learning\n",
        "Repository](https://archive.ics.uci.edu/ml/index.php). Questo dataset contiene una porzione dei dati (anonimizzati) del censimento americano del 1994. Per ciascuna persona, include informazioni quali: età, educazione, stato sociale, occupazione e il guadagno dell'anno.\n",
        "\n",
        "L'**obiettivo** sarà quello di costruire e allenare, tramite l'utilizzo di **Keras**, una rete neurale profonda (Deep Neural Network, DNN) che predica qualora una persona abbia un guadagno minore o maggiore di $50,000 all'anno (**target label**) basandoci sulle altre informazioni delle persone (**features**).\n",
        "\n",
        "*Disclaimer: l'obiettivo di questo tutorial è esclusivamente quello di mostrare tecniche per lo sviluppo di modelli di Machine Learning a scopo didattico. L'autore non si assume responsabilità riguardo le problematiche di fairness che sorgono dall'utilizzo del dataset sopra descritto.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUJY28InQITn"
      },
      "source": [
        "# Import librerie e definizione costanti\n",
        "\n",
        "First, import Python libraries required for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCp8VnyxVppJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7dd09f-8358-4282-a348-f7a72c4c57fc"
      },
      "source": [
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(__import__('sys').version)\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "2.7.0\n",
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2j320zLaAZf"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In questa sezione creiamo una data pipeline per scaricare i dati, processarli, standardizzarli e riportarli in un formato che sia utilizzabile dalla nostra rete neurale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awP73EKYhW2l"
      },
      "source": [
        "Download URL e gestione dei path del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JfirVx2ZAh3"
      },
      "source": [
        "DATA_DIR = './sample_data/census_data'\n",
        "DATA_URL = 'https://storage.googleapis.com/cloud-samples-data/ai-platform/census/data'\n",
        "TRAINING_FILE = 'adult.data.csv'\n",
        "EVAL_FILE = 'adult.test.csv'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xm-S2k8ofm8"
      },
      "source": [
        "Informazioni sulle colonne del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9XeVpjojow"
      },
      "source": [
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "_LABEL_COLUMN = 'income_bracket'\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    'workclass': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
        "        'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
        "    ]),\n",
        "    'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
        "        'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'\n",
        "    ]),\n",
        "    'occupation': pd.api.types.CategoricalDtype([\n",
        "        'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
        "        'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
        "        'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
        "        'Sales', 'Tech-support', 'Transport-moving'\n",
        "    ]),\n",
        "    'relationship': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
        "        'Wife'\n",
        "    ]),\n",
        "    'race': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'\n",
        "    ]),\n",
        "    'native_country': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
        "        'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
        "        'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong', 'Hungary', \n",
        "        'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos', 'Mexico', \n",
        "        'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Poland',\n",
        "        'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand',\n",
        "        'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
        "    ]),\n",
        "    'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
        "        '<=50K', '>50K'\n",
        "    ])\n",
        "}\n",
        "\n",
        "### Hyperparameters for training ###\n",
        "\n",
        "# This the training batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# This is the number of epochs (passes over the full training data)\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Define learning rate.\n",
        "LEARNING_RATE = .01"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZhPlEetcLWi"
      },
      "source": [
        "## Data Loading\n",
        "Creiamo la funzione per scaricare il dataset e riportarli in un formato utile per il preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7OQyfTUgFAs"
      },
      "source": [
        "def _download_and_clean_file(filename, url):\n",
        "    \"\"\"\n",
        "    Scarica i dati dall'URL e li modifica in modo da coincidere col formato CSV.\n",
        "    Args:\n",
        "      filename: filename dove salvare i dati\n",
        "      url: URL della risorsa da cui scaricare i dati\n",
        "    \"\"\"\n",
        "    temp_file, _ = urllib.request.urlretrieve(url)\n",
        "    with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
        "        with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
        "            for line in temp_file_object:\n",
        "                line = line.strip()\n",
        "                line = line.replace(', ', ',')\n",
        "                if not line or ',' not in line:\n",
        "                    continue\n",
        "                if line[-1] == '.':\n",
        "                    line = line[:-1]\n",
        "                line += '\\n'\n",
        "                file_object.write(line)\n",
        "    tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "    \"\"\"Scarica il dataset se non è già presente.\n",
        "    Args:\n",
        "      data_dir: directory dalla quale avremo accesso al dataset\n",
        "    \"\"\"\n",
        "    tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "    if not tf.io.gfile.exists(training_file_path):\n",
        "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "    if not tf.io.gfile.exists(eval_file_path):\n",
        "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "    return training_file_path, eval_file_path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7coANIeAGHBF"
      },
      "source": [
        "training_file_path, eval_file_path = download(DATA_DIR)\n",
        "\n",
        "# Se è andato a buon fine, dovrebbe mostrare i files adult.data.csv and adult.test.csv\n",
        "!ls -l $DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxa7XdwqGR41"
      },
      "source": [
        "Carichiamo i dati in un Pandas Dataframe e diamo una prima occhiata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqg2F2dGWGu"
      },
      "source": [
        "train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "\n",
        "# Questo è il modo in cui appaiono i dati prima della fase di preprocessing\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFonuSecND7"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Il preprocessing è una fase chiave di qualsiasi pipeline di Machine Learning: i dati devono sempre essere puliti, riscalati e resi fruibili per il modello di ML che vogliamo utilizzare. \n",
        "\n",
        "Ricordate il mantra di chiunque faccia analisi dati: garbage-in ➡ garbage-out. 😀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdvKBYPI2eV"
      },
      "source": [
        "### Cleaning\n",
        "\n",
        "Qualsiasi dataset deve passare sempre da una fase di cleaning in cui dobbiamo trattare eventuali valori mancanti, decidere se mantenere gli outliers e scegliere le features utili.\n",
        "\n",
        "In questo caso, possiamo limitarci tratteniamo esclusivamente le colonne utili. Cosa significa \"utili\"? E perché?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCF08Gv9gOnU"
      },
      "source": [
        "UNUSED_COLUMNS = ['fnlwgt', 'education', 'gender']\n",
        "\n",
        "def cleaning(dataframe):\n",
        "    \"\"\"\n",
        "    Rimuove le colonne superflue e codifica i valori categorici.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe with raw data\n",
        "    Returns:\n",
        "      Dataframe with preprocessed data\n",
        "    \"\"\"\n",
        "    return dataframe.drop(columns=UNUSED_COLUMNS)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XnJKn-VKi2x"
      },
      "source": [
        "train_x = cleaning(train_x)\n",
        "eval_x = cleaning(eval_x)\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFO1TYo9ddw"
      },
      "source": [
        "### Codifica dei campi a valori numerici\n",
        "\n",
        "Il dataset deve essere restituito in un formato che sia utilizzabile da una rete neurale, ossia numerico.\n",
        "\n",
        "Nel caso dei **valori numerici**, bisogna effettuare una banale conversione a `float32`.\n",
        "\n",
        "I **valori categorici**, invece, sono definiti come insiemi discreti dove gli elementi definiscono una certa categoria (in senso lato). \\\\\n",
        "La prassi è assegnare a ciascun elemento dell'insieme un codice che lo identifichi univocamente al suo interno.\n",
        "\n",
        "Esempio: `frutta = {mela, pera, banana}` ➡ `frutta = {0, 1, 2}` con `mela=0`, `pera=1`, `banana=2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry9l4Gj39CFE"
      },
      "source": [
        "def columns_to_numeric(dataframe):\n",
        "    # Convertiamo i valori numerici a float32.\n",
        "    numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
        "    dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
        "\n",
        "    # Conversione dei valori categorici a numerici.\n",
        "    cat_columns = dataframe.select_dtypes(['object']).columns\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(_CATEGORICAL_TYPES[x.name]))\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVQE4ITdMUtc"
      },
      "source": [
        "train_x = columns_to_numeric(train_x)\n",
        "eval_x = columns_to_numeric(eval_x)\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBODxNVNgeAW"
      },
      "source": [
        "### Rescaling\n",
        "\n",
        "Due tra le tipologie di rescaling più utilizzate sono le seguenti:\n",
        "* Standardizzazione con z-score: `(x - x_mean)/std` ;\n",
        "* Min-max scaling: `(x_max - x) / (x_max - x_min)` . \n",
        "\n",
        "Il rescaling è una fase fondamentale di qualsiasi data pipeline. Vediamo perché nel caso delle reti neurali."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFVEgt9jg-c1"
      },
      "source": [
        "def standardize(dataframe):\n",
        "    \"\"\"\n",
        "    Riscala le colonne numeriche tramite standardizzazione con z-score.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe\n",
        "    Returns:\n",
        "      Il dataframe in input con le colonne numeriche riscalate.\n",
        "    \"\"\"\n",
        "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
        "    # Normalize numeric columns.\n",
        "    for column, dtype in dtypes:\n",
        "        if dtype == 'float32':\n",
        "            dataframe[column] -= dataframe[column].mean()\n",
        "            dataframe[column] /= dataframe[column].std()\n",
        "    return dataframe"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NXqKVq3NCAn"
      },
      "source": [
        "Dividiamo dati di training e di validazione dal label che vogliamo predire. Il metodo `pop` copia la colonna del label e la rimuove dal dataframe.\n",
        "\n",
        "Dopodiché, facciamo reshaping dei vettori dei label per renderli in un formato `[1 x num_samples]`, che utilizzeremo successivamente in `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8p6iVLojTnt"
      },
      "source": [
        "train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
        "eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
        "\n",
        "train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
        "eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dnWB44AMxqX"
      },
      "source": [
        "Uniamo `train_x` ed `eval_x` per fare la normalizzazione rispetto alla media e alla deviazione standard di tutti dati. Dopodiché li separiamo nuovamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iffqChk3Ms9P"
      },
      "source": [
        "all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
        "all_x = standardize(all_x)\n",
        "train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RA8pTiLuraPS",
        "outputId": "028e146e-c8e9-45cc-bd19-9654bc5a3cc1"
      },
      "source": [
        "train_x.head(), train_y, eval_x.head(), eval_y"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education_num</th>\n",
              "      <th>marital_status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>capital_gain</th>\n",
              "      <th>capital_loss</th>\n",
              "      <th>hours_per_week</th>\n",
              "      <th>native_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.025997</td>\n",
              "      <td>6</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.146933</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.828278</td>\n",
              "      <td>5</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-2.212964</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.046938</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.419265</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.047082</td>\n",
              "      <td>3</td>\n",
              "      <td>-1.197188</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.776285</td>\n",
              "      <td>3</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age  workclass  ...  hours_per_week  native_country\n",
              "0  0.025997          6  ...       -0.034039              38\n",
              "1  0.828278          5  ...       -2.212964              38\n",
              "2 -0.046938          3  ...       -0.034039              38\n",
              "3  1.047082          3  ...       -0.034039              38\n",
              "4 -0.776285          3  ...       -0.034039               4\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZFSL1GtNSNM"
      },
      "source": [
        "Bene, il nostro dataset adesso è in un formato fruibile dalla nostra rete neurale. Passiamo alla parte divertente!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-q7EC93bSOM"
      },
      "source": [
        "# Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLVNks0uX4HK"
      },
      "source": [
        "#### Create training and validation datasets\n",
        "\n",
        "Creiamo la funzione di input per convertire le features e le labels in un \n",
        "[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets) per training e validazione:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N96rtGusATdQ"
      },
      "source": [
        "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
        "    \"\"\"Generates an input function to be used for model training.\n",
        "    Args:\n",
        "      features: numpy array of features used for training or inference\n",
        "      labels: numpy array of labels for each example\n",
        "      shuffle: boolean for whether to shuffle the data or not (set True for\n",
        "        training, False for evaluation)\n",
        "      num_epochs: number of epochs to provide the data for\n",
        "      batch_size: batch size for training\n",
        "    Returns:\n",
        "      A tf.data.Dataset that can provide data to the Keras model for training or\n",
        "        evaluation\n",
        "    \"\"\"\n",
        "    if labels is None:\n",
        "        inputs = features\n",
        "    else:\n",
        "        inputs = (features, labels)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(features))\n",
        "\n",
        "    # We call repeat after shuffling, rather than before, to prevent separate\n",
        "    # epochs from blending together.\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7USPBPPtjRA"
      },
      "source": [
        "Dopodiché, instanziamo i dataset di training e di validazione. Utilizziamo gli iperparametri `NUM_EPOCHS` e `BATCH_SIZE` definiti precedentemente per definire il modo in cui questi dataset devono fornire gli esempi durante il training loop. Settiamo il dataset di validazione così da fargli fornire tutti gli esempi in un solo step. In questo modo, la validazione viene fatta in un singolo step alla fine di ogni epoca di training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF90l4IWuOzZ"
      },
      "source": [
        "# Passiamo un numpy array alle features utilizzando il metodo del Dataframe .values\n",
        "training_dataset = input_fn(features=train_x.values, \n",
        "                    labels=train_y, \n",
        "                    shuffle=True, \n",
        "                    num_epochs=NUM_EPOCHS, \n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "num_eval_examples = eval_x.shape[0]\n",
        "\n",
        "validation_dataset = input_fn(features=eval_x.values, \n",
        "                    labels=eval_y, \n",
        "                    shuffle=False, \n",
        "                    num_epochs=NUM_EPOCHS, \n",
        "                    batch_size=num_eval_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5gTaAhRWkFN"
      },
      "source": [
        "##Design della Deep Neural Network tramite [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)\n",
        "\n",
        "Come descritto dal nome stesso dell'API, Keras Sequential API fornisce un'astrazione che permette di costruire una rete neurale come una semplice sequenza di [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n",
        "\n",
        "La nostra istanza di rete neurale ha diversi hidden layers, e l'ultimo utilizza la sigmoide come funzione di attivazione e fornisce in output un valore tra 0 e 1. In particolare:\n",
        "* Il layer di input ha 100 unità e utilizza la ReLU come funzione di attivazione;\n",
        "* Il primo hidden layer ha 100 unità e utilizza la ReLU come funzione di attivazione;\n",
        "* Il secondo hidden layer ha 75 unità e utilizza la ReLU come funzione di attivazione;\n",
        "* Il terzo hidden layer ha 50 unità e utilizza la ReLU come funzione di attivazione;\n",
        "* Il quarto hidden layer ha 25 unità e utilizza la ReLU come funzione di attivazione;\n",
        "* Il layer di output ha 1 unità e utilizza la sigmoide come funzione di attivazione.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OilROaL1WQ5l"
      },
      "source": [
        "def create_keras_model(input_dim):\n",
        "    \"\"\"Creates Keras Model for Binary Classification.\n",
        "    The single output node + Sigmoid activation makes this a Logistic\n",
        "    Regression.\n",
        "    Args:\n",
        "      input_dim: How many features the input has\n",
        "      learning_rate: Learning rate for training\n",
        "    Returns:\n",
        "      The compiled Keras model (still needs to be trained)\n",
        "    \"\"\"\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            Dense(100, activation=tf.nn.relu, kernel_initializer='uniform',\n",
        "                  input_shape=(input_dim,)),\n",
        "            Dense(75, activation=tf.nn.relu),\n",
        "            Dense(50, activation=tf.nn.relu),\n",
        "            Dense(25, activation=tf.nn.relu),\n",
        "            Dense(1, activation=tf.nn.sigmoid)\n",
        "        ])\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5uPMCCWaeI"
      },
      "source": [
        "Creiamo il modello Keras ed esaminiamo la sua struttura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPRxx3OLWXvx"
      },
      "source": [
        "num_train_examples, input_dim = train_x.shape\n",
        "print('Number of features: {}'.format(input_dim))\n",
        "print('Number of examples: {}'.format(num_train_examples))\n",
        "\n",
        "keras_model = create_keras_model(\n",
        "    input_dim=input_dim,\n",
        "    learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Diamo un'occhiata dettagliata al modello\n",
        "keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8IzzIcrlFV"
      },
      "source": [
        "## Design dell'ottimizzatore\n",
        "\n",
        "L'ottimizzatore rappresenta la cosiddetta *update rule* tramite la quale, ad ogni step di training, aggiorniamo i pesi del modello. In questa fase le scelte da fare sono le seguenti:\n",
        "* Scegliere la funzione di **loss** da utilizzare, ovvero la funzione che valuta il livello di errore che sta commettendo la rete nelle predizioni. In questo caso utilizziamo la binary cross-entropy, che è la più indicata per task di classificazione binaria;\n",
        "* Scegliere il tipo di ottimizzatore. In questo caso, RMSProp;\n",
        "* Scegliere gli iperparametri dell'ottimizzatore. Qui, esclusivamente il learning rate e i parametri di decay del learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLvjJ33eq3t-"
      },
      "source": [
        "A questo punto, creiamo anche l'ottimizzatore e lo linkamo al modello tramite il metodo `model.compile`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMX4eeTvqybH"
      },
      "source": [
        "def link_optimizer(learning_rate):\n",
        "    # Custom Optimizer:\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
        "\n",
        "    # Compile Keras model\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n31oqOgJbP6l"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "from . import model\n",
        "from . import util\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    \"\"\"Argument parser.\n",
        "    Returns:\n",
        "      Dictionary of arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--job-dir',\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help='local or GCS location for writing checkpoints and exporting '\n",
        "             'models')\n",
        "    parser.add_argument(\n",
        "        '--num-epochs',\n",
        "        type=int,\n",
        "        default=20,\n",
        "        help='number of times to go through the data, default=20')\n",
        "    parser.add_argument(\n",
        "        '--batch-size',\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help='number of records to read during each training step, default=128')\n",
        "    parser.add_argument(\n",
        "        '--learning-rate',\n",
        "        default=.01,\n",
        "        type=float,\n",
        "        help='learning rate for gradient descent, default=.01')\n",
        "    parser.add_argument(\n",
        "        '--verbosity',\n",
        "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
        "        default='INFO')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    \"\"\"Trains and evaluates the Keras model.\n",
        "    Uses the Keras model defined in model.py and trains on data loaded and\n",
        "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
        "    format to the path defined in part by the --job-dir argument.\n",
        "    Args:\n",
        "      args: dictionary of arguments - see get_args() for details\n",
        "    \"\"\"\n",
        "\n",
        "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
        "\n",
        "    # dimensions\n",
        "    num_train_examples, input_dim = train_x.shape\n",
        "    num_eval_examples = eval_x.shape[0]\n",
        "\n",
        "    # Create the Keras Model\n",
        "    keras_model = model.create_keras_model(\n",
        "        input_dim=input_dim, learning_rate=args.learning_rate)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    training_dataset = model.input_fn(\n",
        "        features=train_x.values,\n",
        "        labels=train_y,\n",
        "        shuffle=True,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=args.batch_size)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    validation_dataset = model.input_fn(\n",
        "        features=eval_x.values,\n",
        "        labels=eval_y,\n",
        "        shuffle=False,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=num_eval_examples)\n",
        "\n",
        "    # Setup Learning Rate decay.\n",
        "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
        "        verbose=True)\n",
        "\n",
        "    # Setup TensorBoard callback.\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
        "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
        "        histogram_freq=1)\n",
        "\n",
        "    # Train model\n",
        "    keras_model.fit(\n",
        "        training_dataset,\n",
        "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
        "        epochs=args.num_epochs,\n",
        "        validation_data=validation_dataset,\n",
        "        validation_steps=1,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
        "\n",
        "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
        "    tf.keras.experimental.export_saved_model(keras_model, export_path)\n",
        "    print('Model exported to: {}'.format(export_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
        "    train_and_evaluate(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u37CQGQQbwTm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}