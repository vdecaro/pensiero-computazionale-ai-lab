{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_census_tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7SW9SUkpJcCx6NMsf0Rk2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urZOIwgHXBLB"
      },
      "source": [
        "#Tutorial overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCp8VnyxVppJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2j320zLaAZf"
      },
      "source": [
        "# Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scQbpc2Hb312"
      },
      "source": [
        "## Boilerplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awP73EKYhW2l"
      },
      "source": [
        "Download URL e gestione dei path del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JfirVx2ZAh3"
      },
      "source": [
        "DATA_DIR = './sample_data/census_data'\n",
        "DATA_URL = ('https://storage.googleapis.com/cloud-samples-data/ai-platform/census/data')\n",
        "TRAINING_FILE = 'adult.data.csv'\n",
        "EVAL_FILE = 'adult.test.csv'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xm-S2k8ofm8"
      },
      "source": [
        "Informazioni sulle colonne del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9XeVpjojow"
      },
      "source": [
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "UNUSED_COLUMNS = ['fnlwgt', 'education', 'gender']\n",
        "_LABEL_COLUMN = 'income_bracket'\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    'workclass': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
        "        'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
        "    ]),\n",
        "    'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
        "        'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'\n",
        "    ]),\n",
        "    'occupation': pd.api.types.CategoricalDtype([\n",
        "        'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
        "        'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
        "        'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
        "        'Sales', 'Tech-support', 'Transport-moving'\n",
        "    ]),\n",
        "    'relationship': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
        "        'Wife'\n",
        "    ]),\n",
        "    'race': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'\n",
        "    ]),\n",
        "    'native_country': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
        "        'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
        "        'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong',\n",
        "        'Hungary',\n",
        "        'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos',\n",
        "        'Mexico',\n",
        "        'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines',\n",
        "        'Poland',\n",
        "        'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand',\n",
        "        'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
        "    ]),\n",
        "    'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
        "        '<=50K', '>50K'\n",
        "    ])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZhPlEetcLWi"
      },
      "source": [
        "## Data Loading and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7OQyfTUgFAs"
      },
      "source": [
        "def _download_and_clean_file(filename, url):\n",
        "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\n",
        "    The CSVs may use spaces after the comma delimters (non-standard) or include\n",
        "    rows which do not represent well-formed examples. This function strips out\n",
        "    some of these problems.\n",
        "    Args:\n",
        "      filename: filename to save url to\n",
        "      url: URL of resource to download\n",
        "    \"\"\"\n",
        "    temp_file, _ = urllib.request.urlretrieve(url)\n",
        "    with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
        "        with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
        "            for line in temp_file_object:\n",
        "                line = line.strip()\n",
        "                line = line.replace(', ', ',')\n",
        "                if not line or ',' not in line:\n",
        "                    continue\n",
        "                if line[-1] == '.':\n",
        "                    line = line[:-1]\n",
        "                line += '\\n'\n",
        "                file_object.write(line)\n",
        "    tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "    \"\"\"Downloads census data if it is not already present.\n",
        "    Args:\n",
        "      data_dir: directory where we will access/save the census data\n",
        "    \"\"\"\n",
        "    tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "    if not tf.io.gfile.exists(training_file_path):\n",
        "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "    if not tf.io.gfile.exists(eval_file_path):\n",
        "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "    return training_file_path, eval_file_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFonuSecND7"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBODxNVNgeAW"
      },
      "source": [
        "### Standardizzazione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFVEgt9jg-c1"
      },
      "source": [
        "def standardize(dataframe):\n",
        "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
        "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
        "    deviation becomes 1. This can help the model converge during training.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe\n",
        "    Returns:\n",
        "      Input dataframe with the numerical columns scaled to z-scores\n",
        "    \"\"\"\n",
        "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
        "    # Normalize numeric columns.\n",
        "    for column, dtype in dtypes:\n",
        "        if dtype == 'float32':\n",
        "            dataframe[column] -= dataframe[column].mean()\n",
        "            dataframe[column] /= dataframe[column].std()\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCF08Gv9gOnU"
      },
      "source": [
        "def preprocess(dataframe):\n",
        "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe with raw data\n",
        "    Returns:\n",
        "      Dataframe with preprocessed data\n",
        "    \"\"\"\n",
        "    dataframe = dataframe.drop(columns=UNUSED_COLUMNS)\n",
        "\n",
        "    # Convert integer valued (numeric) columns to floating point\n",
        "    numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
        "    dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
        "\n",
        "    # Convert categorical columns to numeric\n",
        "    cat_columns = dataframe.select_dtypes(['object']).columns\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(\n",
        "        _CATEGORICAL_TYPES[x.name]))\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teArSS9NgkF0"
      },
      "source": [
        "### Codifica dei valori categorici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxEs4CunjO64"
      },
      "source": [
        "## Mettiamo tutto insieme "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8p6iVLojTnt"
      },
      "source": [
        "\"\"\"Loads data into preprocessed (train_x, train_y, eval_y, eval_y)\n",
        "dataframes.\n",
        "Returns:\n",
        "    A tuple (train_x, train_y, eval_x, eval_y), where train_x and eval_x are\n",
        "    Pandas dataframes with features for training and train_y and eval_y are\n",
        "    numpy arrays with the corresponding labels.\n",
        "\"\"\"\n",
        "# Download Census dataset: Training and eval csv files.\n",
        "training_file_path, eval_file_path = download(DATA_DIR)\n",
        "\n",
        "train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "\n",
        "train_df = preprocess(train_df)\n",
        "eval_df = preprocess(eval_df)\n",
        "\n",
        "# Split train and eval data with labels. The pop method copies and removes\n",
        "# the label column from the dataframe.\n",
        "train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
        "eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
        "\n",
        "# Join train_x and eval_x to normalize on overall means and standard\n",
        "# deviations. Then separate them again.\n",
        "all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
        "all_x = standardize(all_x)\n",
        "train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
        "\n",
        "# Reshape label columns for use with tf.data.Dataset\n",
        "train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
        "eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlMzaoBujlPv"
      },
      "source": [
        "Alla fine di questo processo, le variabili\n",
        "\n",
        "```\n",
        "train_x, train_y\n",
        "eval_x, eval_y\n",
        "```\n",
        "rappresenteranno i nostri dati di training (features e label) e i dati di validazione (features e label).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-q7EC93bSOM"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n31oqOgJbP6l"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "from . import model\n",
        "from . import util\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    \"\"\"Argument parser.\n",
        "    Returns:\n",
        "      Dictionary of arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--job-dir',\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help='local or GCS location for writing checkpoints and exporting '\n",
        "             'models')\n",
        "    parser.add_argument(\n",
        "        '--num-epochs',\n",
        "        type=int,\n",
        "        default=20,\n",
        "        help='number of times to go through the data, default=20')\n",
        "    parser.add_argument(\n",
        "        '--batch-size',\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help='number of records to read during each training step, default=128')\n",
        "    parser.add_argument(\n",
        "        '--learning-rate',\n",
        "        default=.01,\n",
        "        type=float,\n",
        "        help='learning rate for gradient descent, default=.01')\n",
        "    parser.add_argument(\n",
        "        '--verbosity',\n",
        "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
        "        default='INFO')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    \"\"\"Trains and evaluates the Keras model.\n",
        "    Uses the Keras model defined in model.py and trains on data loaded and\n",
        "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
        "    format to the path defined in part by the --job-dir argument.\n",
        "    Args:\n",
        "      args: dictionary of arguments - see get_args() for details\n",
        "    \"\"\"\n",
        "\n",
        "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
        "\n",
        "    # dimensions\n",
        "    num_train_examples, input_dim = train_x.shape\n",
        "    num_eval_examples = eval_x.shape[0]\n",
        "\n",
        "    # Create the Keras Model\n",
        "    keras_model = model.create_keras_model(\n",
        "        input_dim=input_dim, learning_rate=args.learning_rate)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    training_dataset = model.input_fn(\n",
        "        features=train_x.values,\n",
        "        labels=train_y,\n",
        "        shuffle=True,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=args.batch_size)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    validation_dataset = model.input_fn(\n",
        "        features=eval_x.values,\n",
        "        labels=eval_y,\n",
        "        shuffle=False,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=num_eval_examples)\n",
        "\n",
        "    # Setup Learning Rate decay.\n",
        "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
        "        verbose=True)\n",
        "\n",
        "    # Setup TensorBoard callback.\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
        "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
        "        histogram_freq=1)\n",
        "\n",
        "    # Train model\n",
        "    keras_model.fit(\n",
        "        training_dataset,\n",
        "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
        "        epochs=args.num_epochs,\n",
        "        validation_data=validation_dataset,\n",
        "        validation_steps=1,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
        "\n",
        "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
        "    tf.keras.experimental.export_saved_model(keras_model, export_path)\n",
        "    print('Model exported to: {}'.format(export_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
        "    train_and_evaluate(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u37CQGQQbwTm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}