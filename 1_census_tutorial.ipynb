{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_census_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBHMlASYZJYtyHiX/TVa2s"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urZOIwgHXBLB"
      },
      "source": [
        "#Tutorial overview\n",
        "\n",
        "L'obiettivo di questo tutorial √® quello di mostrare come si costruisce una semplice pipeline di machine learning, partendo dal caricamento dati fino al training e l'inferenza di un modello.\n",
        "\n",
        "## Tematiche\n",
        "In questo tutorial vedremo:\n",
        "* Download e processing dei dati;\n",
        "* Design e training di un modello tramite l'utilizzo di Keras;\n",
        "* Visualizzare il training del modello.\n",
        "\n",
        "## Caso d'uso\n",
        "Il **caso d'uso** su cui lavoreremo si basa sul [United States Census Income\n",
        "Dataset](https://archive.ics.uci.edu/ml/datasets/census+income), fornito dalla [UC Irvine Machine Learning\n",
        "Repository](https://archive.ics.uci.edu/ml/index.php). Questo dataset contiene una porzione dei dati (anonimizzati) del censimento americano del 1994. Per ciascuna persona, include informazioni quali: et√†, educazione, stato sociale, occupazione e il guadagno dell'anno.\n",
        "\n",
        "L'**obiettivo** sar√† quello di costruire e allenare, tramite l'utilizzo di **Keras**, una rete neurale profonda (Deep Neural Network, DNN) che predica qualora una persona abbia un guadagno minore o maggiore di $50,000 all'anno (**target label**) basandoci sulle altre informazioni delle persone (**features**).\n",
        "\n",
        "*Disclaimer: l'obiettivo di questo tutorial √® esclusivamente quello di mostrare tecniche per lo sviluppo di modelli di Machine Learning a scopo didattico. L'autore non si assume responsabilit√† riguardo le problematiche di fairness che sorgono dall'utilizzo del dataset sopra descritto.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUJY28InQITn"
      },
      "source": [
        "# Import librerie e definizione costanti\n",
        "\n",
        "First, import Python libraries required for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCp8VnyxVppJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7dd09f-8358-4282-a348-f7a72c4c57fc"
      },
      "source": [
        "import os\n",
        "from six.moves import urllib\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "print(__import__('sys').version)\n",
        "print(tf.__version__)\n",
        "print(tf.keras.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "2.7.0\n",
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2j320zLaAZf"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "In questa sezione creiamo una data pipeline per scaricare i dati, processarli, standardizzarli e riportarli in un formato che sia utilizzabile dalla nostra rete neurale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awP73EKYhW2l"
      },
      "source": [
        "Download URL e gestione dei path del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JfirVx2ZAh3"
      },
      "source": [
        "DATA_DIR = './sample_data/census_data'\n",
        "DATA_URL = 'https://storage.googleapis.com/cloud-samples-data/ai-platform/census/data'\n",
        "TRAINING_FILE = 'adult.data.csv'\n",
        "EVAL_FILE = 'adult.test.csv'\n",
        "TRAINING_URL = '%s/%s' % (DATA_URL, TRAINING_FILE)\n",
        "EVAL_URL = '%s/%s' % (DATA_URL, EVAL_FILE)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xm-S2k8ofm8"
      },
      "source": [
        "Informazioni sulle colonne del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9XeVpjojow"
      },
      "source": [
        "_CSV_COLUMNS = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
        "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
        "    'income_bracket'\n",
        "]\n",
        "\n",
        "_LABEL_COLUMN = 'income_bracket'\n",
        "\n",
        "_CATEGORICAL_TYPES = {\n",
        "    'workclass': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Federal-gov', 'Local-gov', 'Never-worked', 'Private', 'Self-emp-inc',\n",
        "        'Self-emp-not-inc', 'State-gov', 'Without-pay'\n",
        "    ]),\n",
        "    'marital_status': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Divorced', 'Married-AF-spouse', 'Married-civ-spouse',\n",
        "        'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed'\n",
        "    ]),\n",
        "    'occupation': pd.api.types.CategoricalDtype([\n",
        "        'Adm-clerical', 'Armed-Forces', 'Craft-repair', 'Exec-managerial',\n",
        "        'Farming-fishing', 'Handlers-cleaners', 'Machine-op-inspct',\n",
        "        'Other-service', 'Priv-house-serv', 'Prof-specialty', 'Protective-serv',\n",
        "        'Sales', 'Tech-support', 'Transport-moving'\n",
        "    ]),\n",
        "    'relationship': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Husband', 'Not-in-family', 'Other-relative', 'Own-child', 'Unmarried',\n",
        "        'Wife'\n",
        "    ]),\n",
        "    'race': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Amer-Indian-Eskimo', 'Asian-Pac-Islander', 'Black', 'Other', 'White'\n",
        "    ]),\n",
        "    'native_country': pd.api.types.CategoricalDtype(categories=[\n",
        "        'Cambodia', 'Canada', 'China', 'Columbia', 'Cuba', 'Dominican-Republic',\n",
        "        'Ecuador', 'El-Salvador', 'England', 'France', 'Germany', 'Greece',\n",
        "        'Guatemala', 'Haiti', 'Holand-Netherlands', 'Honduras', 'Hong', 'Hungary', \n",
        "        'India', 'Iran', 'Ireland', 'Italy', 'Jamaica', 'Japan', 'Laos', 'Mexico', \n",
        "        'Nicaragua', 'Outlying-US(Guam-USVI-etc)', 'Peru', 'Philippines', 'Poland',\n",
        "        'Portugal', 'Puerto-Rico', 'Scotland', 'South', 'Taiwan', 'Thailand',\n",
        "        'Trinadad&Tobago', 'United-States', 'Vietnam', 'Yugoslavia'\n",
        "    ]),\n",
        "    'income_bracket': pd.api.types.CategoricalDtype(categories=[\n",
        "        '<=50K', '>50K'\n",
        "    ])\n",
        "}\n",
        "\n",
        "### Hyperparameters for training ###\n",
        "\n",
        "# This the training batch size\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# This is the number of epochs (passes over the full training data)\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Define learning rate.\n",
        "LEARNING_RATE = .01"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZhPlEetcLWi"
      },
      "source": [
        "## Data Loading\n",
        "Creiamo la funzione per scaricare il dataset e riportarli in un formato utile per il preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7OQyfTUgFAs"
      },
      "source": [
        "def _download_and_clean_file(filename, url):\n",
        "    \"\"\"\n",
        "    Scarica i dati dall'URL e li modifica in modo da coincidere col formato CSV.\n",
        "    Args:\n",
        "      filename: filename dove salvare i dati\n",
        "      url: URL della risorsa da cui scaricare i dati\n",
        "    \"\"\"\n",
        "    temp_file, _ = urllib.request.urlretrieve(url)\n",
        "    with tf.io.gfile.GFile(temp_file, 'r') as temp_file_object:\n",
        "        with tf.io.gfile.GFile(filename, 'w') as file_object:\n",
        "            for line in temp_file_object:\n",
        "                line = line.strip()\n",
        "                line = line.replace(', ', ',')\n",
        "                if not line or ',' not in line:\n",
        "                    continue\n",
        "                if line[-1] == '.':\n",
        "                    line = line[:-1]\n",
        "                line += '\\n'\n",
        "                file_object.write(line)\n",
        "    tf.io.gfile.remove(temp_file)\n",
        "\n",
        "\n",
        "def download(data_dir):\n",
        "    \"\"\"Scarica il dataset se non √® gi√† presente.\n",
        "    Args:\n",
        "      data_dir: directory dalla quale avremo accesso al dataset\n",
        "    \"\"\"\n",
        "    tf.io.gfile.makedirs(data_dir)\n",
        "\n",
        "    training_file_path = os.path.join(data_dir, TRAINING_FILE)\n",
        "    if not tf.io.gfile.exists(training_file_path):\n",
        "        _download_and_clean_file(training_file_path, TRAINING_URL)\n",
        "\n",
        "    eval_file_path = os.path.join(data_dir, EVAL_FILE)\n",
        "    if not tf.io.gfile.exists(eval_file_path):\n",
        "        _download_and_clean_file(eval_file_path, EVAL_URL)\n",
        "\n",
        "    return training_file_path, eval_file_path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7coANIeAGHBF"
      },
      "source": [
        "training_file_path, eval_file_path = download(DATA_DIR)\n",
        "\n",
        "# Se √® andato a buon fine, dovrebbe mostrare i files adult.data.csv and adult.test.csv\n",
        "!ls -l $DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxa7XdwqGR41"
      },
      "source": [
        "Carichiamo i dati in un Pandas Dataframe e diamo una prima occhiata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqg2F2dGWGu"
      },
      "source": [
        "train_df = pd.read_csv(training_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "eval_df = pd.read_csv(eval_file_path, names=_CSV_COLUMNS, na_values='?')\n",
        "\n",
        "# Questo √® il modo in cui appaiono i dati prima della fase di preprocessing\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIFonuSecND7"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Il preprocessing √® una fase chiave di qualsiasi pipeline di Machine Learning: i dati devono sempre essere puliti, riscalati e resi fruibili per il modello di ML che vogliamo utilizzare. \n",
        "\n",
        "Ricordate il mantra di chiunque faccia analisi dati: garbage-in ‚û° garbage-out. üòÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYdvKBYPI2eV"
      },
      "source": [
        "### Cleaning\n",
        "\n",
        "Qualsiasi dataset deve passare sempre da una fase di cleaning in cui dobbiamo trattare eventuali valori mancanti, decidere se mantenere gli outliers e scegliere le features utili.\n",
        "\n",
        "In questo caso, possiamo limitarci tratteniamo esclusivamente le colonne utili. Cosa significa \"utili\"? E perch√©?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCF08Gv9gOnU"
      },
      "source": [
        "UNUSED_COLUMNS = ['fnlwgt', 'education', 'gender']\n",
        "\n",
        "def cleaning(dataframe):\n",
        "    \"\"\"\n",
        "    Rimuove le colonne superflue e codifica i valori categorici.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe with raw data\n",
        "    Returns:\n",
        "      Dataframe with preprocessed data\n",
        "    \"\"\"\n",
        "    return dataframe.drop(columns=UNUSED_COLUMNS)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XnJKn-VKi2x"
      },
      "source": [
        "train_x = cleaning(train_x)\n",
        "eval_x = cleaning(eval_x)\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFO1TYo9ddw"
      },
      "source": [
        "### Codifica dei campi a valori numerici\n",
        "\n",
        "Il dataset deve essere restituito in un formato che sia utilizzabile da una rete neurale, ossia numerico.\n",
        "\n",
        "Nel caso dei **valori numerici**, bisogna effettuare una banale conversione a `float32`.\n",
        "\n",
        "I **valori categorici**, invece, sono definiti come insiemi discreti dove gli elementi definiscono una certa categoria (in senso lato). \\\\\n",
        "La prassi √® assegnare a ciascun elemento dell'insieme un codice che lo identifichi univocamente al suo interno.\n",
        "\n",
        "Esempio: `frutta = {mela, pera, banana}` ‚û° `frutta = {0, 1, 2}` con `mela=0`, `pera=1`, `banana=2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry9l4Gj39CFE"
      },
      "source": [
        "def columns_to_numeric(dataframe):\n",
        "    # Convertiamo i valori numerici a float32.\n",
        "    numeric_columns = dataframe.select_dtypes(['int64']).columns\n",
        "    dataframe[numeric_columns] = dataframe[numeric_columns].astype('float32')\n",
        "\n",
        "    # Conversione dei valori categorici a numerici.\n",
        "    cat_columns = dataframe.select_dtypes(['object']).columns\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.astype(_CATEGORICAL_TYPES[x.name]))\n",
        "    dataframe[cat_columns] = dataframe[cat_columns].apply(lambda x: x.cat.codes)\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVQE4ITdMUtc"
      },
      "source": [
        "train_x = columns_to_numeric(train_x)\n",
        "eval_x = columns_to_numeric(eval_x)\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBODxNVNgeAW"
      },
      "source": [
        "### Rescaling\n",
        "\n",
        "Due tra le tipologie di rescaling pi√π utilizzate sono le seguenti:\n",
        "* Standardizzazione con z-score: `(x - x_mean)/std` ;\n",
        "* Min-max scaling: `(x_max - x) / (x_max - x_min)` . \n",
        "\n",
        "Il rescaling √® una fase fondamentale di qualsiasi data pipeline. Vediamo perch√© nel caso delle reti neurali."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFVEgt9jg-c1"
      },
      "source": [
        "def standardize(dataframe):\n",
        "    \"\"\"\n",
        "    Riscala le colonne numeriche tramite standardizzazione con z-score.\n",
        "    Args:\n",
        "      dataframe: Pandas dataframe\n",
        "    Returns:\n",
        "      Il dataframe in input con le colonne numeriche riscalate.\n",
        "    \"\"\"\n",
        "    dtypes = list(zip(dataframe.dtypes.index, map(str, dataframe.dtypes)))\n",
        "    # Normalize numeric columns.\n",
        "    for column, dtype in dtypes:\n",
        "        if dtype == 'float32':\n",
        "            dataframe[column] -= dataframe[column].mean()\n",
        "            dataframe[column] /= dataframe[column].std()\n",
        "    return dataframe"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NXqKVq3NCAn"
      },
      "source": [
        "Dividiamo dati di training e di validazione dal label che vogliamo predire. Il metodo `pop` copia la colonna del label e la rimuove dal dataframe.\n",
        "\n",
        "Dopodich√©, facciamo reshaping dei vettori dei label per renderli in un formato `[1 x num_samples]`, che utilizzeremo successivamente in `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8p6iVLojTnt"
      },
      "source": [
        "train_x, train_y = train_df, train_df.pop(_LABEL_COLUMN)\n",
        "eval_x, eval_y = eval_df, eval_df.pop(_LABEL_COLUMN)\n",
        "\n",
        "train_y = np.asarray(train_y).astype('float32').reshape((-1, 1))\n",
        "eval_y = np.asarray(eval_y).astype('float32').reshape((-1, 1))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dnWB44AMxqX"
      },
      "source": [
        "Uniamo `train_x` ed `eval_x` per fare la normalizzazione rispetto alla media e alla deviazione standard di tutti dati. Dopodich√© li separiamo nuovamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iffqChk3Ms9P"
      },
      "source": [
        "all_x = pd.concat([train_x, eval_x], keys=['train', 'eval'])\n",
        "all_x = standardize(all_x)\n",
        "train_x, eval_x = all_x.xs('train'), all_x.xs('eval')\n",
        "train_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RA8pTiLuraPS",
        "outputId": "028e146e-c8e9-45cc-bd19-9654bc5a3cc1"
      },
      "source": [
        "train_x.head(), train_y, eval_x.head(), eval_y"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>education_num</th>\n",
              "      <th>marital_status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>capital_gain</th>\n",
              "      <th>capital_loss</th>\n",
              "      <th>hours_per_week</th>\n",
              "      <th>native_country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.025997</td>\n",
              "      <td>6</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0.146933</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.828278</td>\n",
              "      <td>5</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-2.212964</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.046938</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.419265</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.047082</td>\n",
              "      <td>3</td>\n",
              "      <td>-1.197188</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.776285</td>\n",
              "      <td>3</td>\n",
              "      <td>1.136580</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.144792</td>\n",
              "      <td>-0.217132</td>\n",
              "      <td>-0.034039</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        age  workclass  ...  hours_per_week  native_country\n",
              "0  0.025997          6  ...       -0.034039              38\n",
              "1  0.828278          5  ...       -2.212964              38\n",
              "2 -0.046938          3  ...       -0.034039              38\n",
              "3  1.047082          3  ...       -0.034039              38\n",
              "4 -0.776285          3  ...       -0.034039               4\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZFSL1GtNSNM"
      },
      "source": [
        "Bene, il nostro dataset adesso √® in un formato fruibile dalla nostra rete neurale. Passiamo alla parte divertente!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-q7EC93bSOM"
      },
      "source": [
        "# Training pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLVNks0uX4HK"
      },
      "source": [
        "#### Create training and validation datasets\n",
        "\n",
        "Creiamo la funzione di input per convertire le features e le labels in un \n",
        "[`tf.data.Dataset`](https://www.tensorflow.org/guide/datasets) per training e validazione:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N96rtGusATdQ"
      },
      "source": [
        "def input_fn(features, labels, shuffle, num_epochs, batch_size):\n",
        "    \"\"\"Generates an input function to be used for model training.\n",
        "    Args:\n",
        "      features: numpy array of features used for training or inference\n",
        "      labels: numpy array of labels for each example\n",
        "      shuffle: boolean for whether to shuffle the data or not (set True for\n",
        "        training, False for evaluation)\n",
        "      num_epochs: number of epochs to provide the data for\n",
        "      batch_size: batch size for training\n",
        "    Returns:\n",
        "      A tf.data.Dataset that can provide data to the Keras model for training or\n",
        "        evaluation\n",
        "    \"\"\"\n",
        "    if labels is None:\n",
        "        inputs = features\n",
        "    else:\n",
        "        inputs = (features, labels)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(features))\n",
        "\n",
        "    # We call repeat after shuffling, rather than before, to prevent separate\n",
        "    # epochs from blending together.\n",
        "    dataset = dataset.repeat(num_epochs)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7USPBPPtjRA"
      },
      "source": [
        "Dopodich√©, instanziamo i dataset di training e di validazione. Utilizziamo gli iperparametri `NUM_EPOCHS` e `BATCH_SIZE` definiti precedentemente per definire il modo in cui questi dataset devono fornire gli esempi durante il training loop. Settiamo il dataset di validazione cos√¨ da fargli fornire tutti gli esempi in un solo step. In questo modo, la validazione viene fatta in un singolo step alla fine di ogni epoca di training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF90l4IWuOzZ"
      },
      "source": [
        "# Passiamo un numpy array alle features utilizzando il metodo del Dataframe .values\n",
        "training_dataset = input_fn(features=train_x.values, \n",
        "                    labels=train_y, \n",
        "                    shuffle=True, \n",
        "                    num_epochs=NUM_EPOCHS, \n",
        "                    batch_size=BATCH_SIZE)\n",
        "\n",
        "num_eval_examples = eval_x.shape[0]\n",
        "\n",
        "validation_dataset = input_fn(features=eval_x.values, \n",
        "                    labels=eval_y, \n",
        "                    shuffle=False, \n",
        "                    num_epochs=NUM_EPOCHS, \n",
        "                    batch_size=num_eval_examples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5gTaAhRWkFN"
      },
      "source": [
        "##Design della Deep Neural Network tramite [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)\n",
        "\n",
        "Come descritto dal nome stesso dell'API, Keras Sequential API fornisce un'astrazione che permette di costruire una rete neurale come una semplice sequenza di [layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n",
        "\n",
        "La nostra istanza di rete neurale ha diversi hidden layers, e l'ultimo utilizza la sigmoide come funzione di attivazione e fornisce in output un valore tra 0 e 1. In particolare:\n",
        "* Il layer di input ha 100 unit√† e utilizza la ReLU come funzione di attivazione;\n",
        "* Il primo hidden layer ha 100 unit√† e utilizza la ReLU come funzione di attivazione;\n",
        "* Il secondo hidden layer ha 75 unit√† e utilizza la ReLU come funzione di attivazione;\n",
        "* Il terzo hidden layer ha 50 unit√† e utilizza la ReLU come funzione di attivazione;\n",
        "* Il quarto hidden layer ha 25 unit√† e utilizza la ReLU come funzione di attivazione;\n",
        "* Il layer di output ha 1 unit√† e utilizza la sigmoide come funzione di attivazione.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OilROaL1WQ5l"
      },
      "source": [
        "def create_keras_model(input_dim):\n",
        "    \"\"\"Creates Keras Model for Binary Classification.\n",
        "    The single output node + Sigmoid activation makes this a Logistic\n",
        "    Regression.\n",
        "    Args:\n",
        "      input_dim: How many features the input has\n",
        "      learning_rate: Learning rate for training\n",
        "    Returns:\n",
        "      The compiled Keras model (still needs to be trained)\n",
        "    \"\"\"\n",
        "    Dense = tf.keras.layers.Dense\n",
        "    model = tf.keras.Sequential(\n",
        "        [\n",
        "            Dense(100, activation=tf.nn.relu, kernel_initializer='uniform',\n",
        "                  input_shape=(input_dim,)),\n",
        "            Dense(75, activation=tf.nn.relu),\n",
        "            Dense(50, activation=tf.nn.relu),\n",
        "            Dense(25, activation=tf.nn.relu),\n",
        "            Dense(1, activation=tf.nn.sigmoid)\n",
        "        ])\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5uPMCCWaeI"
      },
      "source": [
        "Creiamo il modello Keras ed esaminiamo la sua struttura:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPRxx3OLWXvx"
      },
      "source": [
        "num_train_examples, input_dim = train_x.shape\n",
        "print('Number of features: {}'.format(input_dim))\n",
        "print('Number of examples: {}'.format(num_train_examples))\n",
        "\n",
        "keras_model = create_keras_model(\n",
        "    input_dim=input_dim,\n",
        "    learning_rate=LEARNING_RATE)\n",
        "\n",
        "# Diamo un'occhiata dettagliata al modello\n",
        "keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8IzzIcrlFV"
      },
      "source": [
        "## Design dell'ottimizzatore\n",
        "\n",
        "L'ottimizzatore rappresenta la cosiddetta *update rule* tramite la quale, ad ogni step di training, aggiorniamo i pesi del modello. In questa fase le scelte da fare sono le seguenti:\n",
        "* Scegliere la funzione di **loss** da utilizzare, ovvero la funzione che valuta il livello di errore che sta commettendo la rete nelle predizioni. In questo caso utilizziamo la binary cross-entropy, che √® la pi√π indicata per task di classificazione binaria;\n",
        "* Scegliere il tipo di ottimizzatore. In questo caso, RMSProp;\n",
        "* Scegliere gli iperparametri dell'ottimizzatore. Qui, esclusivamente il learning rate e i parametri di decay del learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLvjJ33eq3t-"
      },
      "source": [
        "A questo punto, creiamo anche l'ottimizzatore e lo linkamo al modello tramite il metodo `model.compile`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMX4eeTvqybH"
      },
      "source": [
        "def link_optimizer(learning_rate):\n",
        "    # Custom Optimizer:\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=learning_rate)\n",
        "\n",
        "    # Compile Keras model\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n31oqOgJbP6l"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "from . import model\n",
        "from . import util\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    \"\"\"Argument parser.\n",
        "    Returns:\n",
        "      Dictionary of arguments.\n",
        "    \"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        '--job-dir',\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help='local or GCS location for writing checkpoints and exporting '\n",
        "             'models')\n",
        "    parser.add_argument(\n",
        "        '--num-epochs',\n",
        "        type=int,\n",
        "        default=20,\n",
        "        help='number of times to go through the data, default=20')\n",
        "    parser.add_argument(\n",
        "        '--batch-size',\n",
        "        default=128,\n",
        "        type=int,\n",
        "        help='number of records to read during each training step, default=128')\n",
        "    parser.add_argument(\n",
        "        '--learning-rate',\n",
        "        default=.01,\n",
        "        type=float,\n",
        "        help='learning rate for gradient descent, default=.01')\n",
        "    parser.add_argument(\n",
        "        '--verbosity',\n",
        "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
        "        default='INFO')\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def train_and_evaluate(args):\n",
        "    \"\"\"Trains and evaluates the Keras model.\n",
        "    Uses the Keras model defined in model.py and trains on data loaded and\n",
        "    preprocessed in util.py. Saves the trained model in TensorFlow SavedModel\n",
        "    format to the path defined in part by the --job-dir argument.\n",
        "    Args:\n",
        "      args: dictionary of arguments - see get_args() for details\n",
        "    \"\"\"\n",
        "\n",
        "    train_x, train_y, eval_x, eval_y = util.load_data()\n",
        "\n",
        "    # dimensions\n",
        "    num_train_examples, input_dim = train_x.shape\n",
        "    num_eval_examples = eval_x.shape[0]\n",
        "\n",
        "    # Create the Keras Model\n",
        "    keras_model = model.create_keras_model(\n",
        "        input_dim=input_dim, learning_rate=args.learning_rate)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    training_dataset = model.input_fn(\n",
        "        features=train_x.values,\n",
        "        labels=train_y,\n",
        "        shuffle=True,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=args.batch_size)\n",
        "\n",
        "    # Pass a numpy array by passing DataFrame.values\n",
        "    validation_dataset = model.input_fn(\n",
        "        features=eval_x.values,\n",
        "        labels=eval_y,\n",
        "        shuffle=False,\n",
        "        num_epochs=args.num_epochs,\n",
        "        batch_size=num_eval_examples)\n",
        "\n",
        "    # Setup Learning Rate decay.\n",
        "    lr_decay_cb = tf.keras.callbacks.LearningRateScheduler(\n",
        "        lambda epoch: args.learning_rate + 0.02 * (0.5 ** (1 + epoch)),\n",
        "        verbose=True)\n",
        "\n",
        "    # Setup TensorBoard callback.\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
        "        os.path.join(args.job_dir, 'keras_tensorboard'),\n",
        "        histogram_freq=1)\n",
        "\n",
        "    # Train model\n",
        "    keras_model.fit(\n",
        "        training_dataset,\n",
        "        steps_per_epoch=int(num_train_examples / args.batch_size),\n",
        "        epochs=args.num_epochs,\n",
        "        validation_data=validation_dataset,\n",
        "        validation_steps=1,\n",
        "        verbose=1,\n",
        "        callbacks=[lr_decay_cb, tensorboard_cb])\n",
        "\n",
        "    export_path = os.path.join(args.job_dir, 'keras_export')\n",
        "    tf.keras.experimental.export_saved_model(keras_model, export_path)\n",
        "    print('Model exported to: {}'.format(export_path))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    tf.compat.v1.logging.set_verbosity(args.verbosity)\n",
        "    train_and_evaluate(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u37CQGQQbwTm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}